{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1baf53-eb3b-45af-b420-4a95bc1e25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94ea3f-f443-4795-aea8-784d0a249f62",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c2701-805b-42d1-8b8c-fc07509ed17d",
   "metadata": {},
   "source": [
    "In the previous lectures, we dove into NumPy and its ``ndarray`` object, which provides efficient storage and manipulation of dense typed arrays in Python. Here we'll build on this knowledge by looking at the data structures provided by the Pandas library. Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a ``DataFrame``. ``DataFrame``s are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs.\n",
    "\n",
    "As we saw, NumPy's ``ndarray`` data structure provides essential features for the type of clean, well-organized data typically seen in numerical computing tasks. While it serves this purpose very well, its limitations become clear when we need more flexibility (e.g., attaching labels to data, working with missing data, etc.) and when attempting operations that do not map well to element-wise broadcasting (e.g., groupings, pivots, etc.), each of which is an important piece of analyzing the less structured data available in many forms in the world around us. Pandas, and in particular its ``Series`` and ``DataFrame`` objects, builds on the NumPy array structure and provides efficient access to these sorts of \"data munging\" tasks that occupy much of a data scientist's time.\n",
    "\n",
    "Here we will focus on the mechanics of using ``Series``, ``DataFrame``, and related structures effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac24cb-e811-43c6-b29d-6837faca1e94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading Data\n",
    "\n",
    "Accessing data is a critical first step in any data science project. Many times data can be obtained in simple delimited (e.g., comma, tab) formats that are relatively easy to read in using functionality built into the Pandas module. There are other formats that data could be in (e.g., json, excel, netCDF, etc.), but we won't focus on those in this course.\n",
    "\n",
    "Pandas Tutorial on Reading Data: https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html\n",
    "\n",
    "Curated data sets are often easy to read as they have well defined column headers and don't require use of any special handling to successfully read them into a notebook.\n",
    "\n",
    "## DataFrame\n",
    "The data object that tabular data is read into with Pandas is known as a `DataFrame` (a commonly used variable name to represent this is `df`). A DataFrame represents a rectangular table of data and contains an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.). The DataFrame has both a row and column index; it can be thought of as a dict of `Series` all sharing the same index. Under the hood, the data is stored as one or more two-dimensional blocks rather than a list, dict, or some other collection of one-dimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb43ad2-522d-44e4-b345-ff411512aebe",
   "metadata": {},
   "source": [
    "## Reading a File\n",
    "\n",
    "There are a number of read methods available through Pandas, the most common of which is `read_csv`. This function has robust ability to read in comma (and other) delimited files quite easily. We'll currated data will have a single header row, use commas for delimiting different columns and have one type of data for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f6568-7c0e-4d5d-97f3-a5507bf308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://datahub.io/machine-learning/iris/r/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1781e1f-a42c-4618-bd6c-a12984a072bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "952dd914-e9f0-4a7f-a6ac-11a87e474e69",
   "metadata": {},
   "source": [
    "## Access Column Data\n",
    "\n",
    "There are two methods to access a column of data (a Pandas Series) using a dictionary-like call or a \"dot\" method. To get the `'class'` column from the Iris dataset you would be able to do so via:\n",
    "\n",
    "Dictionary-like\n",
    "\n",
    "`df['class']`\n",
    "\n",
    "or\n",
    "\n",
    "\"Dot\" Method\n",
    "\n",
    "`df.class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f601ff5-cf60-47de-9228-91d6b9075a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e93fa81-f6e0-4af0-b289-ecf1c651638d",
   "metadata": {},
   "source": [
    "To get the Numpy array from the series, call `.values` on the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44d614-1bf9-4a21-b23f-df2f28d976f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4bd9e2-c14a-44d0-a151-8b2672a19d9c",
   "metadata": {},
   "source": [
    "## Reading a New Dataset\n",
    "\n",
    "Some datasets may contain metadata or additional information somewhere within a file you are attempting to read in. This will likely cause your data to be read in a manner that won't allow it to be successfully used. To demonstrate the use of keyword arguments in reading in a dataset, we'll use the Southern Oscillation Index (SOI) dataset, which is a measure related to El Nino events.\n",
    "\n",
    "The raw dataset can be found at: https://www.cpc.ncep.noaa.gov/data/indices/soi\n",
    "\n",
    "Sometimes the way we have read in similar data just won't work. We have to move to an alternative. What do you think are difficulties we would have reading in the SOI data set based on the output above and inspecting the data via the link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1126d-7d14-4b6b-a85f-53243845fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soi = pd.read_fwf('https://www.cpc.ncep.noaa.gov/data/indices/soi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c678e6-bcfc-4412-a35e-cd6f84d8b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e59956cd-cdcf-4eb4-b2ec-8e26b9b52dc5",
   "metadata": {},
   "source": [
    "## Accessing Rows and Columns\n",
    "\n",
    "By setting the `index_col` keyword argument when reading in to our data, we can create some more logical values for us to use, which will make subsetting our data by rows easier.\n",
    "\n",
    "To access a row, you'll need to use the `.loc` method on the DataFrame, then use the appropriate index values to access a given row or rows. Since we made the years the index values, we'll be able to use a year value as an integer value to pull it from the DataFrame. For example, if you had a DataFrame, `df`, with years as the index values, you would pull out 1956 as:\n",
    "\n",
    "```python\n",
    "df.loc[1956]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9eb052-e619-4f9e-af45-dcd156b87a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1086a72-59e2-4a0c-8b03-d93ac56e9326",
   "metadata": {},
   "source": [
    "Then we can perform many of the same method calculations on a DataFrame or Series that we can with a Numpy array. For example, we can use the `.mean()` method for comuting the mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318359d-ec3b-4700-ad5c-9ad69b1e7bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1624f512-5bdc-42e0-8452-4e3d7d4814b8",
   "metadata": {},
   "source": [
    "## Add Calculated Column\n",
    "Let's calculate the average SOI value for each year and add it as a new column to the DataFrame.\n",
    "\n",
    "We can use either methods built into the DataFrame itself or use Numpy functions to aid in the calculation. If we wanted to perform a calculation for each row we can add the keyword argument `axis=1` or if we wanted to do it for each column, then we would add the keyword argument `axis=0` and we would implicitly loop over each row or column, respectively.\n",
    "\n",
    "Let's compute the average for each year and add a new column to our DataFrame called `avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280a50c-cfaa-46bc-8398-cea6a0f67469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f3c1e-830e-41dc-8e5b-e5bae91d36ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaad0715-1712-4649-bc95-17692117ea1f",
   "metadata": {},
   "source": [
    "## Operating on Null Values\n",
    "\n",
    "As we have seen, Pandas treats ``None`` and ``NaN`` as essentially interchangeable for indicating missing or null values.\n",
    "To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures.\n",
    "They are:\n",
    "\n",
    "- ``isnull()``: Generate a boolean mask indicating missing values\n",
    "- ``notnull()``: Opposite of ``isnull()``\n",
    "- ``dropna()``: Return a filtered version of the data\n",
    "- ``fillna()``: Return a copy of the data with missing values filled or imputed\n",
    "\n",
    "We will conclude this section with a brief exploration and demonstration of these routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01803587-4df4-46a3-bf41-db7c9bde5438",
   "metadata": {},
   "source": [
    "### Detecting null values\n",
    "Pandas data structures have two useful methods for detecting null data: ``isnull()`` and ``notnull()``.\n",
    "Either one will return a Boolean mask over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499406a2-df0b-4a83-b060-ff30ca199b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5912928-0cdb-4ff9-af77-6440d4009bdc",
   "metadata": {},
   "source": [
    "### Dropping null values\n",
    "\n",
    "In addition to the masking used before, there are the convenience methods, ``dropna()``\n",
    "(which removes NA values) and ``fillna()`` (which fills in NA values). For a ``Series``,\n",
    "the result is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cc556-cf00-4ead-9781-997b8cb3c656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c12afaaf-58f5-42ae-8b81-f01816c77464",
   "metadata": {},
   "source": [
    "We cannot drop single values from a ``DataFrame``; we can only drop full rows or full columns.\n",
    "Depending on the application, you might want one or the other, so ``dropna()`` gives a number of options for a ``DataFrame``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1c07b-46eb-409c-82de-1dc5be542423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bec4a390-51df-4f50-9e0b-a3331c2e899b",
   "metadata": {},
   "source": [
    "## Output Refined Datasets\n",
    "We don't ever want to change the raw dataset, so that we can always go back to it in case we need to. Other times we don't want to have to use the time or computer memory to read in ALL of the data, if we are only working with a small subset of rows or columns.\n",
    "\n",
    "So once you have your refined dataset, you may want to save it in a format that you can quickly read it back in to a notebook.\n",
    "\n",
    "As long as your dataset is a DataFrame, there are methods to save in similar formats that Pandas can read including, comma deliminated (csv), excel, and JSON.\n",
    "\n",
    "Let's save our refined SOI dataset as a comma deliminated file with our added columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a3f81-41b0-4d09-afa7-6727a19c9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soi.to_csv('soi_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048513da-eff8-4ad6-8a78-23213d3fb817",
   "metadata": {},
   "source": [
    "## Reading Data Messy Data\n",
    "\n",
    "Data are not necessarily messy on purpose, but that the nature of the data itself bring complications. For example, you may have any combination of missing data or mixed data types in a column that are non-standard values. So how do we handle this?\n",
    "\n",
    "https://www.rcc-acis.org/docs_webservices.html#title36\n",
    "\n",
    "Data Issues:\n",
    "* No header row\n",
    "* Missing values labeled as `M`\n",
    "* Precipitation, Snow, and Snowdepth can have trace values `T`\n",
    "* Use dates as an index\n",
    "* Parse date values to be datetime-like and not strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eaa7e4-208f-41fc-ac5c-3735b118e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://data.rcc-acis.org/StnData?sid=ordthr&sdate=por&edate=por'\n",
    "                 '&elems=mint,avgt,maxt,pcpn,snow,snwd&output=csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eee3ae-ab55-4ab4-9423-ec800ba0ade2",
   "metadata": {},
   "source": [
    "Inspect the last couple of rows with the `.tail()` method.\n",
    "\n",
    "For example, the following will give you the last ten rows of your DataFrame,\n",
    "```python\n",
    "df.tail(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82664306-b980-4c7a-9857-f6ce1fbcb9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b73e0829-0faf-463d-a4e6-9b7fd12285ee",
   "metadata": {},
   "source": [
    "Hmmm, we still have the trace (`T`) precipitation amounts in there. Let's go ahead and use the `.replace()` method to put in a very small value for `T`, to ensure we can have a fully numeric column of data.\n",
    "\n",
    "The format of the replace method is:\n",
    "```python\n",
    "df.replace(old_value, new_value)\n",
    "```\n",
    "\n",
    "And then we'll want to force the data to be floating point values. Since there were `T`'s in the precipitation, snow, and snow depth columns, all of the values are forced to be strings, so you won't be able to do any calculations on them until you make them integer or floating point values. Here we want floating point values because precipitation can be in non-integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ade3ef-24e7-4dae-8417-a86dec9e08cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "915bc601-b0fa-4503-a5f5-bc1320744e49",
   "metadata": {},
   "source": [
    "## Subest using Datetime\n",
    "\n",
    "Since we did the work to parse the dates of our file and made them the index value, we can more easily subset our DataFrame for any particular timeframe using the datetime module.\n",
    "\n",
    "Let's grab the data from 1991 through 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d3372-895a-405f-a8b8-2a556e2d433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72a348-1f29-4b74-b4c8-7b389f566e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328ec7f5-3445-4c20-9a2d-0d22ff93026d",
   "metadata": {},
   "source": [
    "## Grouby Operations\n",
    "\n",
    "Pandas has some very extensive methods and functions for performing many high-level operations. One really useful one is the `groupby()` method. Here we have subset our data for a decade, but what if we wanted to know what was the average maximum temperature over that decade period?\n",
    "\n",
    "If we only had our data stored in a Numpy array we would have to do some creative subsetting in order to get the right data. We would likely be more than a little flumoxed by having a few leap years in there, which would throw off any easy methods for getting the right data.\n",
    "\n",
    "The Pandas `grouby` method will allow us to group by one or more elements. So to group by the day of the year, we would need to group by both the month and the day. Here we can exploit our index values being dates and use the abiliy to pull both the month and the data from those index values to help us perform our `groupby()` operations.\n",
    "\n",
    "To get the month: `df.index.month`\n",
    "\n",
    "To get the day: `df.index.day`\n",
    "\n",
    "This is similar to working with a `datetime` object and getting out the individual pieces of information. Here we just call our DataFrame index values using the \"dot\" method and since those are datetime-like objects, then we can simply call the appropriate attribute values from those objects.\n",
    "\n",
    "To perform the group by for a single value:\n",
    "\n",
    "```python\n",
    "df.groupby(value1)\n",
    "```\n",
    "\n",
    "or for two variables:\n",
    "\n",
    "```python\n",
    "df.groupby([value1, value2])\n",
    "```\n",
    "\n",
    "Where `value1` and `value2` are each arrays that represent the row information you wish to group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5d4bd-f77d-487b-8e15-3775394612bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = ltm_data.groupby([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ba75b-51ff-4013-ac7f-e3ab8a4bd7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08e4ef44-9623-41ee-bb8b-a8e89589b435",
   "metadata": {},
   "source": [
    "## Rolling Average\n",
    "\n",
    "Pandas also provides a simple method for calculating a rolling average. For example, if you wanted to computing a seven day rolling average with a minimum period in an average of one:\n",
    "\n",
    "```python\n",
    "df.rolling(window=7, min_periods=1).mean()\n",
    "```\n",
    "\n",
    "Here the `.rolling()` method creates a copy of the DataFrame, organizing the data as needed, but to get the actual rolling mean, you would still need to perform the `.mean()` calulcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5209599-f57a-42db-8b6b-28205a1b02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtemp_rolling_avg = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dfa59-f039-4ca4-a579-0c07a9a92449",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Now let's use our plotting knowledge to plot some of our grouped values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868629b-2af0-4554-9ca0-9ff7220dec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.plot(range(1, 367), maxtemp_rolling_avg, label='Rolling MaxT', color='red')\n",
    "\n",
    "\n",
    "ax.set_title('1991-2000 Rolling Average Max Temp KORD')\n",
    "ax.set_ylabel('Temperature (F)')\n",
    "ax.set_xlabel('Julian Day of Year')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efa3e-b241-4d1c-8b8f-fd10e3fb53f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
